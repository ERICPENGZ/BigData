
数据分片与路由

	一.哈希

		1.hash分片方式_RoundRobin
			一层取模

		2.虚拟桶
			Membase：在待存储记录和物理机之间引入了虚拟桶层，第一层：所有记录首先通过哈希函数映射到对应的虚拟桶，记录和虚拟桶是多对一的映射关系。一个backet包含多条record。第二层：虚拟桶到物理机，通过查表来实现的。Membase通过内存表来管理这层映射关系。

		3.一致哈希性算法
			分布式哈希表DHT用到的一种实现方式。
			m = 5，哈希空间：0~31.

		4.范围分片
			首先将所有记录的主键key进行排序，然后再排好序的主键空间里将记录划分成数据分片，每个数据分片存储有序的主键空间片段内的所有记录。在实现时，保持一个分片的映射表，表的每一项记录数据分片的最小主键及其对应的物理主机地址。

==================================================

数据复制与一致性

	一.基本原则与设计理念

		1.CAP_不可兼得
			强一致性（更新操作的效果与只有单份数据是一致的），可用性（限定延时），分区容忍性。
			CP：数据无副本，满足高可用和分区容忍
			AC：没有网络分区，满足同时更新和延时限定
			AP：跟AC存在权衡。存在P（发生了网络分区导致两台机器无法通信），则返回数据必然会加长延时或者存在不一致性。

		2.三个步骤
			1.首先识别网络分区的发生。2.进入分区的模式。3.恢复数据的一致性和弥补分区模式中产生的错误。

		3.ACID_针对数据库系统
			1.原子性：事务全部执行，要么不执行
			2.一致性：A+B=100，A改，B也要响应修改。满足一致性约束
			3.事务独立（隔离性）：事务之间需要序列化执行。事务执行时互不影响。
			4.持久性：运行成功后，对系统的更新状态是永久的，不会无故的回滚

		4.Base原则_针对云平台/nosql系统
			1.基本可用：允许偶尔的fail
			2.软状态：不要求完全一致，处于有状态和无状态之间的中间属性。
			3.最终一致：在给定的时间窗口内达到一致
			summary：牺牲了强一致性来保持高可用性。

		5.CAP/ACID/BASE的关系
			1.ACID和CAP中c,ACID中的C的一致性要求比CAP中的广。ACID：数据操作的一致性，在出现网络分区的时候不可能完成。CAP：强一致性，多副本对外表现为单副本
			2.当出现网络分区时，ACID中的隔离性要求只能在一个分区内执行：因为数据序列化需要通信。
			3.当出现分区时，每个分区尽可能的执行CAP原则

		6.幂等性
			f(f(x)) = f(x);
			调用方反复执行同一个操作与执行一次操作的效果相同。在调用方执行操作，但因为网络原因无法得到调用成功的响应时，会认为调用失败而在此调用。


	二.一致性模型
	---------------------
		一致性关系图(p41)
		
		1.强一致性：
			如果某个进程对数据进行了更新，那么以后的后续的操作都会以这个更新后的值为基准，直到这个数据被其他进程改变为止

		2.最终一致性：
			在一个时间片段后可以保证强一致性。不一致窗口：为了达到高可用的性，同一份数据通常会被存储到多个节点，不同进程可能操作数据的不同备份，当某进程对数据做出了修改以后，需要一定时间来将这个新值传递给其他节点。

		3.因果一致性：
			A,B,C三个进程，A在修改了数据后，以notify()的形式通知B进程数据已被修改，则B以后都用这个新值，则A,B存在因果一致性。但C仍可能读到A的旧值

		4.读你所写一致性：
			3的特例：A给自己发notify，其他进程不收影响，可能读到旧值

		5.会话一致性：
			在同一个会话内，保持4一致

		6.单调读一致：
			某个进程一旦读到新值，它将不会再读到老值

		7.单调写一致：

	三.副本更新策略
	-----------------
		1.同时更新：
			情形1：不通过一致性协议同时更新，但会出现数据不一致（每个机器接受到的update顺序可能不一样）
			情形2：一致性协议处理请求顺序，可以保证一致性，但延时会增加

		2.主从更新：
			数据的副本存在master和从副本。则对数据的更新操作首先提交到master，再用master更新到从副本。如果存在多个更新顺序。则主副本决定更新顺序，从副本也遵从这个更新顺序。根据master通知从副本的不同机制来分为三个情形：

				情形1：同步，master等待所有从副本更新完成后才确认更新操作完成。保证强一致性，但存在较大的延时。

				情形2：异步，master在通知之前就可确认更新操作，但为了防止master崩掉，会记录更新操作。请求延时和一致性的权衡取决于读的响应方式：
						A:所有读请求必须先交给master
						B:从可以响应，但会带来不一致问题（zookeeper）

				情形3：混合，master同步更新部分从节点，卡夫卡

		3.任意节点更新：
			任意节点都可应响应请求。


	四.一致性协议
	-------------------
		1.两段提交协议：
			两个实体：一个协调者，多个参与者。协调者：分布式事务的特殊的管理协调作用
			顾名思义：提交划分为两个阶段：1.表决阶段 2.提交阶段
					A:表决阶段
						1.协调者视角：向所有参与者发送vote_request消息
						2.参与者视角：收到vote_request后返回vote_commit消息，表示已经做好准备，否则返回vote_abort,告知协调者现在不能提交事务的：“可能”
					B:提交阶段
						【协调者视角】：收集A阶段的表决信息。若一致可以提交：最终提交。返回global_commit			    ,通知参与者本地提交。若不一致：取消事务，向所有参与者广播 				 global_abort通知事务取消
						[参与者视角]：取决于收到的协调者发来的信息
			有限状态机 p48

			应对阻塞：超时判断机制。参与者互询机制。（原理）
					 超时判断：
					 		A:协调者处于wait时间过长（没收到所有的返回消息），强制中断
					 		B：参与者init：等待协调者vote_request消息，超时，简单的本地中断事务，发	送vote_abord消息
					 参与者互询机制：
					 		参与者ready的情况（不知道外面什么情况），需要问同伴
					 		A：同伴已经收到commit了，则自身转为commit
					 		B：同伴是abort，转为abort
					 		C：同伴init，abort
					 		D：同伴也ready了，再问其他同伴
					 		summary：只要同伴是commit，init，abort，自己就能做出判断，若都是ready，			则崩掉


		2.向量时钟：
			分布式环境下生成时间之间偏序关系的算法，偏序关系代表时间发生的先后顺序导致的事件间因果依赖关系语义，通过时间戳和事件绑定可以用来判断事件之间的因果性。

			假设分布式系统里有n个进程，每个进程i维护一个向量,VCi[1....n]，初始值为都0.
			VCi[j]:表示进程i看到进程j的逻辑时钟。
			更新规则	：
				1.每当进程i产生了以下3种时间（发送消息，接收消息，内部时间），则VCi[i]+=1
				2.进程i发送消息m，将自己的VCi和m都发出去，即为m_vc
				3.当进程j接收到i发送过来的消息时，更新每一位数值的规则：
									VCj[x] = max(VCj[x],m_VCi[x]),x=1.....n

			因果推断：
				（对任意K,E_VC[k]<=F_VC[k]） and (存在K,使得E_VC[k]<F_VC[k]),E是F的原因

			应用：数据版本的控制

		3.RWN协议：
			 N：在分布式下，有多少备份数据
			 W：一次成功的更新操作至少有W份数据写入成功
			 R：一次成功的读操作至少有W份数据写入成功
			 若  R+W>N,则满足数据一致性协议（R和W一定存在重叠，即一定会至少有个机器读到最新的程序）

			 变种：若N = 4，可以采用R = 3，W = 1的设置，这样便适用于 写入速度要求快，读要求不高的程序

		Paxos协议：略
		Raft协议：略

---------------------------------------------------------------

大数据常用算法与数据结构
	一：布隆过滤器
			常用来监测某个元素是否是巨量数据集合中的成员。

		1.基本原理：
			准备：1.长度为m的数组[1.......m] 2.k个hash函数,h1.....hk
			过程：将元素a依次代入每个hash，得到k个值，则数组对应k个位置的树置为1。
			判断：对元素b同样适用k次hash，若在这个过程中得到的值所对应的数组的数全为1，则b属于这个集合
				  否则，若出现一次0，则排除这个集合

		2.误判
			成员不在集合，但判定位在集合中，因此适用于允许发生一定程度的误判的场景中
			但不会漏判，即在集合中一定会给出正确判断
			误判率：
				1.集合大小n，hash函数个数k，位数组m。
				2.  正比   	 均衡问题	   反比
			在实际应用中：设定好误判率，已知集合大小，算出数组长度m（m = -(nlnp)/(ln2)^2）

		3.改进过滤器
			删除问题-->无法删除的问题-->计数BF（一个信息单位由多个比特位表示，加入时，逐步加一，在删除时，只要对应位置都不为0，则在集合，可以删除）
			代价：m*比特位数

		4.应用

	二：SkipList
			依靠随机数生成来保持一定概率保持数据的均衡分布。O(logN)->insert,delete,select
			Redis在实现sorted set数据结构时采用的是skiplist
			核心思路：在原本的链表中增加跳跃式指针，例如指向后续的后续。
			随机数的作用：随机决定节点应该有多少个指向后续节点的指针，有几个指针就成这个节点是几层的
			
	三：LSM树
			本质：将大量的随机写操作转换成批量的序列写，适合对写操作有高要求的应用场景
			组成：
				内存部分：MEMTable，ImmuTable Memtable
				磁盘部分：Current文件，Manifest文件，log，SSTable文件

				当应用写入一条K,V记录时，会先写入log文件，成功后将记录写入MemTable，成功后基本完成了写入操作 。只涉及：一次磁盘操作(顺序写)和内存操作(写入)。MemTable采用了维护有序记录
				快速插入查找的SkipList数据结构。（是一种高速写入数据结构的主要原因）

			为什么先写入log日志：
				防止系统崩溃后造成数据丢失。数据刚开始是写入内存的，此时如果系统崩溃，内存中的数据还没来得及存入磁盘，所以会产生丢失数据。为了避免这种情况，先写入log文件，再写入内存。

				当MemTable存入的数据到了一定限度时，原来的MemTable会转变为ImmuTable（不可更改）。然后会产生新的MemTable和新的log文件。LevelDb的后台会调度ImmuTable转入到SSTable。SSTale就是内存中的数据不断导出并进行compaction操作形成的。
				Compaction:https://www.cnblogs.com/tekkaman/p/4878868.html

			SSTable:
				主键有序，Level0的多个文件可能存在key重叠，其他level则不会。SST中的某个文件属于特定层级，而且其记录的key是有序的。因此存在Max_key,Min_key。

			Manifest：
				记载了SSTable中各个文件的元信息。

			Current：
				记载了当前可用的Manifest文件。

			LSM树：内存中MemTable+SSTable

			Compaction机制： 
				MinorCompaction：
						MemTable->ImmuTable（多层级SkipList队列）,其记录是根据key有序排列的。

				MinorCompaction实现：
						按照ImmuTable中的记录由小到大依次写入SSTable的新建Level0文件中，写完后建立文件的index数据。对于被删除的记录：做假删除，只记录key作为知道这条记录被删除的标记，真正的删除在更高层的Compaction操作中完成。

				MajorCompaction：
						某个level下SSTable的数量到达一定限制后，LevelDB会从这个level中的SSTbale选择一个文件与更高级level(level+1)的SSTable合并

						两种情况：
							level0和level1	    :level0存在重叠，需要多个ImmuTable参与MarjorCom
							level1和level2.....n

						LevelDB选定合并的文件的技巧：轮流选择，比如：这次是A文件，那么下次就是key与A
						 	相邻的B文件，这样每个文件都有机会和高level的文件合并

						LevelDB在level+1层的选择：选择和A文件有key重叠的所有文件来合并

						合并方法：
							选定文件后，每个文件的key都是有序，合并之后依然有序，同时扔掉那些不再有价值的KV数据
							多路归并排序：依次找出其中最小的key，也就是对多个文件中的所有记录进行重新排序。之后采取一定的标准判断这个key是否保留。就这样对kv数据进行一一处理，形成了新L+1层文件，之前的L层文件和L+1层参与Compation的文件已经没有意义。全部删掉。

				结论：将大量随机写转换为批量的序列写

	四：Merkle哈希树
		用途：在海量数据下快速定位少量变化的数据内容。Git，比特币，比特彗星，nosql都用到了此类技术。

			1.Merkle树原理
				递归哈希。类似于哈夫曼编码的树

			2.Dynamo中的应用
				略

			3.比特币应用
				
---------------------------------------------------
集群资源管理与调度
	一：资源管理抽象模型	

		1.概念模型

			[资源组织模型] ---> [调度策略] ---> [任务组织模型]

			资源组织模型:将集群中当前可用资源按照一定方式组织起来，方便以后的资源分配过程。一个常见的资源组织方式是将资源组织成多层级队				列的方式，例如facebook：all resource --> group --> pool 的三级队列结构。

			调度策略：以一定的方式将资源分配提交到系统的任务。常见的策略：FIFO，公平调度。能力调度，延迟调度。

			任务组织模型：将多用户的提交的多任务通过一定方式组织起来。方便后续资源分配。hadoop1.0将任务按照平级多队列组织。hadoop2.0增				 加了层级队列的树形队列结构。

		2.通用架构

			p88页架构图

			集群中的每台机器会配置节点管理器，职责：不断的向资源管理器汇报目前本机资源使用情况，并负责容器的管理工作。当某个任务。被分配到本节点执行时节点管理器负责将其纳入某个容器执行并对改容器的资源进行隔离，避免不同容器之间相互干扰。

			通用调度器 = 资源收集器 + 资源调度策略 + 管理资源池 + 管理工作队列数据结构

			资源收集器：不断的从集群内各个节点收集和更新资源状态信息，并将最新情况反映到资源池中。

			资源池：列出了目前可用的系统资源

			调度策略：决定如何将资源池中的可用分配资源分配给工作队列。资源调度模块往往是可插拔：也就是个根据不同业务的需求进行相应的选择		 
			工作队列：当用户提交新任务的时候，其进入工作队列等候分配其可启动的资源

	二：调度系统设计的基本问题

		1.资源异构性和工作负载异构性

			异构性：组成整体的元素之间存在着较大的差异

			资源异构性：比如数据中心中的机器的硬件性能参差不齐。在分配时资源时，需要划分到更细小的粒度

			工作负载异构性：不同的业务需求，比如有的业务需要高可用。有的业务需要快响应。在分配时资源时，需要考虑具体的场景需求

		2.数据局部性

			设计的基本原则：将计算任务推送到数据所在地。

			数据局部性 = 节点局部性+机架局部性+全局局部性

			解释：节点局部性，不需要跨机器。 机架局部性：需要跨机器，不需要跨机架。 全局局部性：最大的局部性

		3.抢占式调度和非抢占式调度

			高优先级：抢占式。

			强调公平：非抢占式

		4.资源分配力度

			计算任务 = job级别 + task级别   一个job(作业)由多个并发的task(任务)构成。Task之间的依赖关系通过DAG表示。例如MR

			群体分配：一次性分配完成。 MPI任务

			增量式分配：对于某个作业来说，只要分配部分资源就能启动一些任务开始运行，随着空闲资源的不断增多，可以逐步增量式分配给作业其			   他任务以维持整个作业的向前推进。 例如 MR为代表的批处理任务

			资源储备分配：分配到一定量的资源才启动这个job

		5.饿死和死锁问题

			饿死不一定意味着死锁

			死锁：相互占有资源还不放开，又请求资源，

		6.资源隔离方法

			YARN：采取了将各种资源(CPU,内存，网络带宽，I/O带宽)封装在容器中的细粒度资源分配方法。整个分布式系统资源管理封装
				  了为数众多的资源容器。为了避免不容任务之间相互干扰，需要提供容器间的资源隔离方法。

			常用手段：Linux容器(Linux container).LXC：轻量化的内核虚拟技术，可以用来进行资源和进程运行的隔离，通过LXC可以在一台物理
					 主机上隔离出多个相互隔离的容器。

	三：资源管理与调度系统泛型

		根据宏观运行机制的不同进行分类：集中式调度，两级调度器，状态共享调度器 。P92架构图

		1.集中式调度器

			整个系统缺乏并发性且所有调度逻辑全部由中央调度器来实现，集中调度器又可分为以下两种

			1.1 单路径调度器
				不论是哪种任务类型，都采用统一调度策略来进行资源管理与调度。例如MPI高性能计算系统。

			2.1 多路径调度器
				针对任务类型支持多种调度策略，调度策略还是由中央调度器来实现的。

			总结：缺乏灵活性，可扩展性差，并发性差，适合小规模集群。

		2.两级调度器

			中央调度器 + 框架调度器

			中央调度器(全局可见) ---> 计算框架 --->计算框架根据自身计算任务的特性,使用自身的调度策略来进一步细粒度的划分从中央得来的资
			源。每个框架只能看到中央分配给自己的资源


		3.状态共享调度器

			每个计算框架可以看到整个集群中的所有资源，并相互竞争的方式去获取自己所需的资源，根据自身特性采取不同的具体资源调度策略。同时采用了乐观并发控制手段解决不同框架在竞争过程中的需求冲突 Google的Omega

			与2相比：中央的功能强弱不同。Omega严重弱化中央的功能，只保存可恢复的集群资源状态信息的副本，称为单元状态。每个框架自身内部都		会维护单元状态的一份私有并不断更新的副本信息。

			如果两个不同框架竞争同一份资源，因其决策过程都是在各自的私有数据上做出的，并通过原子事务提交，系统保证只有一个胜出，失败的可以后续继续申请资源。类似于MVCC的乐观并发控制。

		4.归纳

			1.适合小集群，类似完全计划经济
			2.适合负载同质的大规模集群应用场景 类似大政府小市场的混合经济模式
			3.负载异构性较强的且资源冲突不多的大规模集群应用场景	小政府 大市场的自由竞争经济模式

			大规模数据处理系统的一个趋势是在硬件层之上的统一资源管理与系统调度，这是提高集群资源利用率的根本之道。而这个统一的中央应该提供的是一种弱服务，框架调度器具有较大的资源分配自由度。



	四：资源调度策略

		hadoop中已经实现了FIFO，公平调度器，能力调度器

		1.FIFO
			略

		2.公平调度器

			多个用户的多个任务分配到多个资源池。每个资源池设定资源分配的最低保障和最高上限。管理员也可以设定资源池的优先级。

			调度过程：
					1.根据每个pool的最低资源保障，将系统中的部分资源分配给各个资源池
					2.根据pool的优先级将剩余资源按比例分配给各个资源池
					3.在每个pool中按照作业的优先级或者公平调度策略分配资源

			与3的区别：
					1.公平调度器支持抢占式调度策略，保证长时间被分配不到的资源从任务中放出放到资源池
					2.强调作业间的公平性


		3.能力调度器

			更强调用户之间的公平性

			1.将用户和任务组织成多个队列，每个队列可以设定资源最低保障和适用上限
			2.当一个队列的资源有剩余时，可以将剩余资源暂时分配给其他队列。
			3.调度器在调度时，优先将资源分配给	资源使用率最低的队列
			4.队列内部，按照作业优先级的先后顺序FIFO

		4.延迟调度策略

			辅助策略

			对与当前要被分配资源的任务i，如果这个任务不满数据局部性，则资源分配跳过i，分配给其他满足的任务，但是如果i被跳过多次，则放弃数据局部性，被迫接受资源启动当前任务

		5.主资源公平调度策略

			最大最小公平算法：最大化目前分配到最少资源的用户或这任务的资源量

			具体实现：p97


	五：Mesos
		略

	六：YARN
		
		两级调度器。  RM + AM + NM

		RM类似于中央管理器，全局唯一的资源管理器。 AM服务器：每个作业一个  NM：每个工作节点一个

		RM = 调度器 + AM服务器 + Client-RM接口 + RM+NM接口

		调度器：提供公平或者能力调度策略，支持可插拔的方式，系统管理者可以指定全局的资源分配策略。
		
		Client-RM接口:按照一定的协议管理客户提交的作业，

		RM-NM接口：主要和各个工作节点的NM通过 “心跳协议” 进行通信。

		RM：支持抢占式调度 。AM在资源请求信息内可以明确指明数据偏好性。

		典型的AM请求信息：[容器数量，每个容器所含的资源数量，数据局部性偏好，应用内部的任务优先级信息]

		AM：Application Maste：类似jobtracker，负责向RM申请资源，并且管理内部的任务的运行过程。RM的AMS负责为AM申请资源并启动它，使得
			作业能够运行起来，之后任务的管理工作交给AM负责。同时，AM也负责任务间资源分配时的数据局部性等优化调度策略

		NM：负责机器内资源的管理，在NM启动后，向RM注册，同时根据心跳协议定时向RM汇报。同时也接收AM的命令：启动或杀死某容器内的运行的任	    务。

		步骤：
			1.client 向 RM 提交任务
			2.RM启动AM，
			3.AM负责将作业划分为若干的任务，并根据此向RM请求分配资源
			4.启动节点管理器

-------------------------------------------------

第五章：分布式协调系统
		
		Chubby + zookeeper

	5.1 Chubby锁服务

		通过对数据加锁来实现分布式环境下的资源协调问题。

		推选领导者：通过各个服务器竞争某个数据的锁来实现，竞争成功的服务器将信息写入数据同时对其他服务器可见。

		5.1.1 架构

			1.一个数据中心有一个Chubby单元，每个chubby有5个服务器
			2.通过paxos协议选出leader服务器，所有的读/写操作都用leader完成。其余的follower作为备份服务器

			leader：投票选出，存在一定的任期，当任期结束后，如果leader没有发生故障，则会继续担任，负责，系统重新推选新的leader

			follower：若备份服务器长时间发生故障，则Chubby则会更新DNS,用来指向新的机器。leader会周期性的查询DNS，一旦发现备份服务器发	   		  生了变化，则通过一致性协议将这一变化通知给其他follower，新的follower会根据其他follower保存的数据选择自己要维护
		  			  的数据

		  	更新请求：当多数的follower都更新完成后，才认为本次跟新操作完成。当follower接收到更新请求时，返回主控服务器的地址，由其决定			 读写操作

		 5.1.2 数据模型

		 	chubby提供树形结构的文件目录并进行管理

		 5.1.3 会话与keep alive机制

		 	会话机制：client 向服务器发送keepalive（一个RPC调用），服务器在接收到后，阻塞RPC，等待客户端原先的租约接近过期，再释放RPC
		 			 阻塞，keepalive调用返回。


		 5.1.4 客户端缓存

		 	为了减少客户端和服务器之间的通信量，chubby允许客户端在本地缓存部分数据。由chubby维持数据的一致性。为了保持数据的一致性，chubby 维护一个缓存表，记录哪个客户端缓存了哪些信息。当接受到修改数据的请求时，先阻塞这个请求，并查询缓存表，通知所有缓存该数据
		 	的客户端该数据无效。客户端在收到通知后返回给服务器确认信息，当服务器收到所有确认信息时，执行更新操作

	
	5.2 Zookeeper

		5.2.1 架构

			leader + follower结构，读操作任何一个服务器都可以响应，写操作必须经过leader服务器

			ZAB协议：保证更新的顺序性和一致性。一致性：多数投票仲裁。 一致性：从节点和主节点的更新顺序是一样的。多数follower返回更新成功
					，则更新成功。

			防止读过期数据，ZK提供了syn接口，收到接口调用时，从节点向主节点同步状态信息。

			版本控制：ZID编号，服务器在响应读写请求时，都会返回一个zid编号，保证服务器目前所见到的最新的更新版本。

			容错机制：
					1.replay log：先写入外存log避免数据丢失。
					2.模糊快照：周期性对数据进行快照时，不对内存数据进行加锁，利用深度遍历的方式将内存中的树形结构转入外存快照数据中去
					3.zk具有更新数据操作的幂等性。

		5.2.2 数据模型

			类似于chubby，zk的内存储数据模型类似于传统的文件系统模式，由树型的层级目录结构构成，其中的节点称为znode。znode可以是文件，也可以是目录。

			znode分为两类：
					1.持久性znode：显示调用delete操作时才会删除
					2.临时性znode：会话session结束即被zk删除。

			

		5.2.3 API

			同步机制	API + 观察者模式 观察者模式：当节点发生变化时，通知客户端。

		5.2.4 典型应用场景

			1.leader选举：防止主从结构中 单点失效的问题。

					选举机制：1.创建临时节点，存储leader的信息(地址信息+辅助信息)
							 2.每个进程依次从临时节点读取信息
							 3.若果读到信息，则已经有leader，若读取失败，则还没有leader，现在正在读信息的进程自己新创建一个临时节点   ，并且写入自己的信息，称为leader
							 4.zk会通知其他所有follower，leader发生了变化
							 5.如果leader挂掉，则临时节点也会死掉，则按照顺序下一个节点称为leader(一般是按照快慢的顺序，节点下会生成				 /xxx/xx000000001等节点。(增加10个数字,节点上谁快谁的号小,leader是选号小的);)


			2.配置管理

					设置观察者标记，当节点配置信息发生变化时，客户端会受到通知

			3.组成员管理

					服务器的新增与故障离开。当负载过大。动态增加节点实现负载均衡。或者自动清理故障节点。

			4.任务分配

					创建任务管理队列，所有新进入系统的任务，都会在tasks节点下创建子节点。当有新增任务时，监控进程分配给机器i，并且machine会在m-i下创建对应的子节点 task-j,

			5.锁管理

					
			6.barrier实现同步					

		5.2.5 zk的实际应用
			略

----------------------------------------------------------------------------

六：分布式通信

	6.1.序列化与RPC框架

		RPC概念：允许程序调用网络中其他机器中的进程，当A进程调用B进程时，A进程被挂起，B进程执行，A进程可以通过参数向B进程传递参数，同理
				也可以接收B进程返回的数据。

				RPC一般会结合序列化和反序列化操作，实现数据的高效通信。

				框架图P121

			三个常用的框架:Protocol buffer 与 thrift 与 Avro

		6.1.1 Protocol buffer 与 thrift
			略

		6.1.2 Avro
			略

			总结：追求序列化的高效，但不需要RPC机制：PB。需要内建的便捷的RPC支持：Thrift。动态语言的集成：Avro

	6.2 消息队列

		消息传递过程中保存消息的容器或者中间件

		6.2.1常见的消息队列系统

			ActiveMQ，Kafka

			两种队列模式：1. 消息队列模式 2. Pub-Sub模式

						消息队列：消息生产者把消息放入队列，消息接受者将消息取出队列
						pub-sub模式：生产者将消息放入指定的主题中，消费者订阅某个特定的主题，当主题发生变化时，消费者可以通过pull获取			信息。

		6.2.2 kafka

			轻量级的消息系统，提供了消息持久化机制，保证至少一次送达。Pub-Sub机制的分布式消息系统。

			1.整体架构

				生产者 + 代理服务器集群 + 消费者

				生产者产生指定主题的消息 ---> 传入代理服务器集群 ---> 订阅了某个topic的消费者pull取消息

				代理服务器集群：在磁盘中管理各个topic的消息队列

				kafka内部支持对topic进行数据分片，每个数据分片是有序的，不可更改的尾部追加式队列。队列内部的每个消息被分配队列内唯一的ID。生产者在发送消息时可以按照某个规则指定某个topic。

				对于数据分片：以一系列被分割成相等大小的文件来存储。每次追加到尾部。同时维护“每个文件中的首个消息组成的offset数组及其大小信息”。

				读取消息时：kafka会指定消息对应的offset和大小内容信息，根据索引进行二分查找和处理找到对应的起始位置，返回给消费者。

				kafka的消息是存储在外存中的，因此有天然的持久性机制。

				容错性：将管理信息（目前消费者读取到了队列中的哪个消息）放到消费者本地保存；除此之外,很多其他的管理信息也交给zk保存。
						保证了代理服务器的无状态性


				kafka使用zk保存的管理信息实现的功能：

							1.侦测代理服务器和消费者的加入和删除

							2.针对1引起的系统变化，对消息系统负载均衡

							3.维护消费者和消息topic及数据分片的关系，并保存当前消费者读取的消息的offset

							4.数据副本管理信息

			2.ISR副本管理

				高可用：kafka采用消息副本机制，维护一个master副本和slave副本，并且维护的是topic的分片信息而不是队列信息。所有针对这个		   分片的操作都有master副本来响应，slave只负责从master同步更新

				ISR的原因：在Zab和Paxos中，多数投票的机制允许1个副本故障但需要维护3个副本个数，在消息系统中，一般采用3个副本实现高可			  用，则需要的副本个数达到了11个，效率过低

				ISR的运行机制：	将所有次级副本分散到两个集合 ISR集合+非ISR集合。
								
								1.ISR集合中及时和主备份副本数据保持一致，另一个集合允许落后。
								2.主备切换时，只允许从ISR中选取候选主副本。在数据分片写入时，ISR中副本全部写入时，才算写入成功


			3.性能优化

				kafka基于文件系统的消息系统

				针对磁盘的特点，顺序读要比随机读的效率高很多。

				预读+迟写：将整块数据写入也页缓存+将较多的逻辑写操作合并成一个大的物理写操作

				因此：基于磁盘的数据存储与处理系统的提升性能的关键是选择读/写模式，这个原则体现在很多分布式系统的设计思路中

				kafka能高效处理大批量消息的一个重要原因是将读/写操作转换为顺序写，比如类似log文件类型的尾部追加模式。为了增加文件在网络
				传输效率，kafka调用了linux中的sendFile

				正常情况下将文件通过网络传输需要经过的数据通道：

							1.Os将数据从磁盘复制到os内核的页缓存中
							2.应用将数据从内核缓存复制到应用空间的缓存中
							3.应用将数据写回内核中的socket缓冲区
							4.os将数据从socket缓存区复制到网卡的缓存区，然后通过网络发出

					分析：整个数据通路设计4次数据复制+2次系统调用。如果使用sendFile，可以避免多次复制，OS可以直接将数据从内核页缓存复		 制到网卡缓存	，大大加速整个过程


		6.3 应用层多播通信

			将数据通知到网络中的多个接收方-->多播

			在P2P网络中，分布式应用层的节点组织结构有星型结构，DHT环形结构以及树形结构，也有任意节点随意相连的情况。在无结构中，采用相	邻发送的病毒式传播，但会造成消息的指数级别增长，传播效率不高。

			6.3.1 Gossip协议

				P2P环境下的多播通信问题

				1.信息传播模型

					全部通知模型 + 反熵模型 + 散步谣言模型

					全部通知模型：当某个节点有更新消息，立即通知所有其他节点；其他节点在接收到通知后，判断接收到的消息是否比本地消息更				 新（时间戳或者版本信息），如果更新，则更新本地数据。否则，无动于衷。但容错性不佳

					反熵模型：更新的信息经过一定的轮数的传播后，越来越有序，集群内节点都会获得全局最新信息。系统越来越有序

							P节点 一传十 十传百式的更新

							P,Q交换信息的三种方式：

										push + pull + push+pull

					散步谣言模型：被拒绝的次数越多，停止传播的概率就越大。被拒绝：发现Q已经被其他节点更新过来

-----------------------------------------

七：数据通道
	
		一：log日志数据收集

			log数据收集系统的关注点：1）低延迟 2）可扩展性 3）容错性

			7.1.1 Chukwa

				建立在hadoop之上，基本策略：是首先收集大量单机的log增量文件，将其汇总后形成大文件，之后再利用mr任务对齐进行加工处理。Chukwa定位不仅仅在数据收集，也在后端集成数据分析和可视化界面

				整体架构：每台机器节点部署Chukwa代理程序（Agent），其负责收集应用产生的Log数据并通过Http协议传给Chukwa收集器		 ，若对应的收集器发生了损坏，则代理程序可以检查收集器列表并从中选择另外一个收集器来发送数据，这样可以		    实现一定的容错性。收集器负责将数据写入HDFS文件，当一个DataSink文件大小到一定程度后，关闭这个文件开
						启一个新的文件。ArchiveBuilder：进一步合并DataSink文件并做些排序以及去重的工作。Demux是MR程序，负责对原始的log数据进行解析抽取，将无结构记录转换成有结构/半结构化的记录。对于结构化的数据，可以导入数据库
						进行sql分析。

				source产生log数据 -->HTTP post --> Chukwa收集器 --> DataSink Files --> Archive Builder --> Demux -->写入DB
						
				瓶颈：效率在于Demux，主要是因为MR任务启动开销以及中间数据和结果数据多次读写磁盘导致的。


			7.1.2 Scribe

				架构：
					client（Thrift）---> Scribe服务器(维护了消息队列)---> HDFS

		二：数据总线

			作用：形成数据变化通知通道，当集中存储的数据源(RDBMS)的数据发生变化时，能够尽快通知相关应用，使得尽快捕捉这种变化

			数据总线的设计关注3个特性：
					1.近实时性
					2.数据回溯能力 重新指定时刻的历史数据变化情况。（满足至少一次送达协议）
					3.主题订阅能力：将指定主题的数据变化推送给应用，避免浪费资源的全部推送


			如何实现：
					应用双写 + 数据库日志挖掘

					1.应用双写：

						数据库变化同时写入 数据库 + pub-sub消息系统中，关注某和topic的应用可以向消息系统订阅

					2.数据库日志挖掘：

						变更先写入数据库，总线从数据库的日志中挖掘出数据变化信息，然后通知给关心数据变化的各类应用，这样可以保证数据的一致性。

						下面的是基于2的实现

			7.2.1 DataBus

				DB发生变化 --> 内存数据中继器从数据库日志pull最近提交的事务，并将数据转化为序列化格式Avro --> 更新存入环状内存缓冲区域。---> Client 监听到中继器发生变化 并pull取最新的数据到本地 

				pull而不是push:每个客户端处理数据延时大小不一，pull更灵活，可由client自主控制

				因为中继器缓存区大小有限-->引入bootstrap长期存储

				两种情况client会向bootstrap发出请求：
						1）数据积压导致了覆盖，因此向bootstrap发出请求，请求T之后的数据
						2）新加入的client，先获取一份时间T的更新数据库快照，然后按照 1）的方式获取T之后的增量更新数据

				bootstrap机制：像客户端一样侦听中继器的变化，采用logwriter的方式写入增量更新存储区，在具体实现时使用了MYSQL数据库。log applier批量的将更新合并进入快照存储区，形成不同时间点的快照，具体实现快照时是使用文件存储方式。

			7.2.2 Wormhole


		三：数据的导入导出

			sqoop



--------------------------------------------------------------

八：分布式文件系统

		什么时候 11+7=12 13*10=11 3÷7=10

		8.1 Google文件系统GFS

			8.1.1 设计原则

				1.集群的单点故障很常见 2.存储大多是大文件 3.大量追加写操作 4.大量顺序读

			8.1.2 整体架构

				主控服务器 + 众多的chunk服务器 + GFS客户端 

				主控：管理工作 		chunk：实际存储 			GFS：响应读写请求

				文件系统：目录+存放在某个目录下的文件构成的树形结构，在GFS中，树形结构称为"命名空间"。

				在实际存储时：会将文件先切分成块，每一个块称为一个chunk，通常将chunk的大小设定为64MB.这样每个文件就由众多的  			 chunk构成。Chunk服务器内部会对Chunk再进行切割，切割成Block，这是文件读取的一个基本单位。即一次至
							少读一个block。

				因此：GFS命名空间 1：n 目录+GFS文件 1：n 固定大小的Chunk 1：n block

				Chunk是基本的存储单元，block是基本的读取单元

				主控服务器：维护GFS命名空间+Chunk命名空间，因此每个Chunk有不同的编号以便被识别。而且主控还记录了哪个Chunk在哪		   台服务器。 并且维护了文件名称对应多个chunk的映射关系
				Chunk服务器： 负责对Chunk的实际存储，同时响应GFS客户端对自己负责的Chunk读写请求。

				GFS客户端读取数据：
					 计算文件所在哪个chunk，<file名，Chunk序号>。发送给主控，找到所在机器和全局唯一的chunk编号 返回给客户端。
					 然后，client和相应的chunk建立联系

			8.1.3 GFS主控服务器

				维护三类元数据：GFS+Chunk命名空间，文件到chunk的映射关系，chunk所在服务器的存储信息

				前两类管理信息 备份多份记录在日志文件内，并将这个系统日志分别存储在多台机器上。第三类通过定期询问chunk服务器

				工作：管理元信息，创建新的chunk及其备份，负载均衡chunk服务器，垃圾回收

			8.1.4 系统交互行为

				对于写入chunk的操作：GFS必须写入所有chunk的备份，为了维护数据的一致性。在多个备份中，选出一个主备份，管理剩下次备份的写入顺序

				整体流程：
					1.GFS客户端首先和 主控服务器通信，获知Chunk所在的服务器，包括所有备份的服务器地址
					2.客户端将要写入的数据推送给3个备份的chunk
					3.备份chunk首先将这些代写入的数据放在缓存中，然后通知GFS客户端是否接收成功
					4.若所有备份都接收成功，GFS通知主备份，可以进行写入操作
					5.主备份将数据写入chunk中，通知次级备份按照指定顺序写入数据
					6.次级备份写完后通知主备份
					7.主备份通知GFS客户端写入成功
					写操作图P151

			8.1.5 Colossus

				略


		8.2	HDFS（important）

			8.2.1 架构

				HA+namenode HA：解决单点失效问题。 namenode：解决系统水平扩展问题

				datanode*n --心跳协议+块报告--> namenode/secondary namenode

				NameNode:文件目录树结构，文件到数据块block的映射信息，block副本及其存储位置等各种管理数据。这些数据保存在内存		 中,同时，在磁盘保存两个元数据文件 

				fsimage，editlog，fsimage是内存空间元数据在外存的镜像文件。editlog则是各种元数据操作的write-ahead-log文件。
						 在体现到内存数据变化之前首先会将记录计入editlog中以防止数据丢失。两个文件一起可以构造出完整的内存数据

				SecondaryNameNode：定期从NN中拉取fsimage和editlog并进行合并。形成新的fsimage传给NN，减轻NN的工作压力。SNN是					个提	供检查点的服务器。

			8.2.2 HA方案

				ANN + SNN ZK 

			8.2.3 NameNode联盟

				大的namespace 切割成小的namespace分别由每个NN管理

		8.3 HayStack

			对象存储系统 

			
















































































































			




























